üìÑ Chat with Your Docs: Uma Base de Conhecimento com IA (CLI)
Este projeto √© um script de linha de comando inteligente constru√≠do em Python usando LangChain e Ollama. Ele permite processar um conjunto de documentos PDF e fazer perguntas sobre eles, transformando uma pasta de arquivos em uma base de conhecimento consult√°vel.

A aplica√ß√£o utiliza uma arquitetura RAG (Retrieval-Augmented Generation) para encontrar as informa√ß√µes mais relevantes nos seus documentos e gerar uma resposta coesa usando um modelo de linguagem rodando 100% localmente.

‚ú® Features Atuais
Processamento de M√∫ltiplos Documentos: L√™ e processa todos os arquivos PDF de uma pasta especificada.

100% Local e Privado: Utiliza o Ollama para rodar modelos de linguagem (como o gemma3n:e4b) no seu computador. Seus documentos e perguntas nunca saem da sua m√°quina.

Banco de Vetores Persistente: Utiliza o ChromaDB para salvar os embeddings dos documentos em disco (chroma_db/). Isso significa que voc√™ s√≥ precisa processar os PDFs uma vez, tornando as execu√ß√µes futuras muito mais r√°pidas.

Observabilidade com LangSmith: Pronto para ser integrado com o LangSmith para rastrear, depurar e monitorar cada passo da cadeia de RAG.

üõ†Ô∏è Como Funciona
O script segue uma arquitetura cl√°ssica de RAG:

Carregamento de Dados: Os arquivos PDF da pasta designada s√£o carregados na mem√≥ria.

Divis√£o (Chunking): O texto de cada documento √© dividido em peda√ßos menores e gerenci√°veis.

Embedding e Armazenamento: Cada peda√ßo de texto √© convertido em um vetor num√©rico (embedding) usando o modelo all-MiniLM-L6-v2 e armazenado em um banco de dados de vetores local e persistente (ChromaDB).

Recupera√ß√£o (Retrieval): Ao fazer uma pergunta, o script busca os peda√ßos de texto mais relevantes no ChromaDB.

Gera√ß√£o: Os peda√ßos de texto recuperados s√£o enviados como contexto para o modelo de linguagem (via Ollama), que gera a resposta final e a exibe no console.

üöÄ Setup e Instala√ß√£o
Siga os passos abaixo para rodar o projeto localmente.

1. Pr√©-requisitos
Python 3.9+

Ollama instalado e rodando.

Um modelo de linguagem baixado no Ollama (o script usa gemma3n:e4b, ent√£o rode ollama run gemma3n:e4b no seu terminal).

2. Prepare o Projeto
Clone este reposit√≥rio (ou simplesmente use seu script LangChainRAG.py em uma nova pasta).

Crie uma pasta chamada documentos (ou o nome que preferir) e coloque seus arquivos PDF dentro dela.

3. Instale as Depend√™ncias
√â recomendado criar um ambiente virtual:

Bash

python -m venv venv
source venv/bin/activate  # No Windows: venv\Scripts\activate
Instale todas as bibliotecas necess√°rias. Crie um arquivo requirements.txt com o seguinte conte√∫do e depois rode pip install -r requirements.txt.

Plaintext

# Conte√∫do para requirements.txt
langchain
langchain-community
langchain-huggingface
langchain-ollama
python-dotenv
pypdf
chromadb
sentence-transformers
tensorflow # ou torch, dependendo da sua instala√ß√£o
4. Configure o Script e as Vari√°veis de Ambiente
No script LangChainRAG.py, altere a vari√°vel pasta para o caminho da sua pasta de documentos:

Python

pasta = "documentos"  # Exemplo
Para o LangSmith, crie um arquivo chamado .env na raiz do projeto e adicione suas chaves:

Snippet de c√≥digo

# --- Configura√ß√£o do LangSmith (Opcional) ---
LANGCHAIN_TRACING_V2="true"
LANGCHAIN_ENDPOINT="https://api.smith.langchain.com"
LANGCHAIN_API_KEY="SUA_CHAVE_DE_API_DO_LANGSMITH"
LANGCHAIN_PROJECT="Meu-Chat-com-Docs-CLI"
No script LangChainRAG.py, descomente a linha #load_dotenv() para ativar a leitura do arquivo .env.

Œπœá Observabilidade com LangSmith
Este projeto est√° pronto para usar o LangSmith para fornecer total transpar√™ncia sobre o que acontece na execu√ß√£o. Ativando o LangSmith, voc√™ poder√° visualizar o "trace" (rastreamento) de cada pergunta, o que √© inestim√°vel para depurar e entender o fluxo de dados.

üîó Link para o Projeto P√∫blico no LangSmith
Voc√™ pode acompanhar as intera√ß√µes e a performance deste projeto em tempo real atrav√©s do link p√∫blico abaixo:

https://smith.langchain.com/public/9fe16d63-9e03-4f80-b5ed-ac4430703d0d/r

üèÉ Como Rodar o Script
Certifique-se de que o servi√ßo do Ollama est√° rodando.

No script, altere a vari√°vel pergunta para a pergunta que voc√™ deseja fazer sobre seus documentos:

Python

pergunta = "Qual √© a principal conclus√£o do documento X?"
Execute o seguinte comando no seu terminal:

Bash

python LangChainRAG.py
A resposta ser√° impressa diretamente no console.
